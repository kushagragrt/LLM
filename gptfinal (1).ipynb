{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNIKrJklBCG9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "pVdDziwcBHq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "n_embd, n_head, n_layer, dropout:\n",
        "\n",
        "Explanation: These parameters define the architecture of the Transformer-based language model. n_embd is the embedding dimension, n_head is the number of attention heads in the multi-head attention mechanism, n_layer is the number of transformer blocks in the model, and dropout is the probability of dropping out units during training, serving as a regularization technique to prevent overfitting.\n",
        "\n",
        "Transformer-based models, introduced by Vaswani et al., are a type of neural network architecture designed for sequence-to-sequence tasks. They utilize self-attention mechanisms, eliminating the need for recurrence, making them highly parallelizable.\n",
        "\n",
        "An embedding is a vector representation of a discrete item, often used in natural language processing to convert categorical data (like words or characters) into a continuous vector space."
      ],
      "metadata": {
        "id": "LSKcAx-3UKyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su7XZ4uVBPqB",
        "outputId": "90a36cf2-c327-4707-9ee4-14f1a82f9e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e9cf1775690>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK0XXfwrBS9S",
        "outputId": "56ae53d1-eec9-4982-c087-0150dafa3f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-13 08:48:59--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-01-13 08:49:00 (35.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "yw1j_swUBcNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N52wL47WBfVD",
        "outputId": "fc009abc-b0e3-4e26-fd47-971cdb1035e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "Ks4MfSQcBiqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "p9TY-6GQBlv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()# PyTorch decorator used to disable gradient computation temporarily\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "HvLcBnGgBohb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Head class defines one head of self-attention.\n",
        "It has linear layers (key, query, and value) to project the input tensor x into key, query, and value spaces.\n",
        "The tril buffer is a lower triangular matrix used for masking in self-attention.\n",
        "The forward method computes the attention scores, applies masking, performs softmax, and aggregates the values, implementing the self-attention mechanism.\n",
        "In summary, this code implements a self-attention head within the transformer model and provides a function to estimate the average loss on the training and validation sets while temporarily disabling gradient computation during the evaluation.\n"
      ],
      "metadata": {
        "id": "nczZYxYrars0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SELF -ATTENTION MECHANISM**\n",
        "\n"
      ],
      "metadata": {
        "id": "8la99TRprhc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) This involves taking the dot product of the query (q) and key (k) tensors, dividing\n",
        "        # by the square root of the dimension of the key (k), and applying a mask to the lower triangular part of the attention matrix to make future positions attend\n",
        "        #to the past only.\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out"
      ],
      "metadata": {
        "id": "luISS6XeazCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "super().__init__() calls the constructor of the parent class (nn.Module). It's necessary for proper initialization.\n",
        "\n",
        "self.key, self.query, and self.value are linear layers responsible for projecting the input tensor into key, query, and value spaces, respectively. These projections are crucial for the self-attention mechanism.\n",
        "\n",
        "nn.Linear(n_embd, head_size, bias=False) creates a linear layer with no bias term, where n_embd is the input size and head_size is the output size.\n",
        "\n",
        "self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) registers a buffer named tril in the module. Buffers are persistent and included in the state_dict but not considered parameters. In this case, tril is a lower triangular matrix created using torch.tril and initialized with ones.\n",
        "\n",
        "self.dropout = nn.Dropout(dropout) creates a dropout layer with a dropout probability specified by the dropout variable. Dropout is a regularization technique used to prevent overfitting by randomly setting a fraction of input units to zero during training.\n",
        "\n",
        "FORWARD\n",
        "\n",
        "The forward function in the Head class aims to perform self-attention on the input tensor x. Self-attention is a mechanism that allows the model to weigh different positions of the input differently when making predictions, considering dependencies between different positions in the sequence.\n",
        "\n",
        "Here's a breakdown of what the function does:\n",
        "\n",
        "Linear Transformations:\n",
        "\n",
        "The input tensor x undergoes linear transformations using three linear layers (key, query, value). These transformations create three tensors: k (key), q (query), and v (value), each with a size of (batch, time-step, head size).\n",
        "Attention Scores Calculation:\n",
        "\n",
        "Attention scores, often referred to as \"affinities,\" are computed using the dot product between the query (q) and key (k) tensors. The result is scaled by the square root of the dimension of the key (k). This step calculates how much focus each element in the sequence should place on the others. The output is a tensor (wei) of size (batch, time-step, time-step).\n",
        "Masking:\n",
        "\n",
        "The lower triangular part of the attention matrix is set to -inf. This masking ensures that each position attends only to positions at or before it, preventing information flow from future positions to past positions.\n",
        "Softmax Activation:\n",
        "\n",
        "The softmax activation is applied along the last dimension to obtain normalized attention weights (wei). This ensures that the weights sum to 1 along the time-step dimension.\n",
        "Dropout:\n",
        "\n",
        "Dropout is applied to the attention weights for regularization. This helps prevent overfitting by randomly setting some of the weights to zero during training.\n",
        "Weighted Aggregation:\n",
        "\n",
        "The values (v) are linearly transformed, and the result is aggregated based on the computed attention weights (wei). This step calculates a weighted sum of values for each position in the sequence.\n",
        "Output:\n",
        "\n",
        "The final output (out) represents the result of the self-attention mechanism for the given input tensor x"
      ],
      "metadata": {
        "id": "L2VcXwEQbhAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MULTIPLE HEADS OF SELF ATTENTION IN PARALLEL**"
      ],
      "metadata": {
        "id": "H6nj6LIJvLix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)#logits->unnormalised probabilities\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "GYO4kNmHBrF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MultiHeadAttention class is designed to apply multiple heads of self-attention in parallel and then aggregate their outputs. This is a key component of the transformer architecture, allowing the model to capture diverse patterns and dependencies in the input sequence.\n",
        "\n",
        "Here's an explanation of the class:\n",
        "\n",
        "Initialization:\n",
        "\n",
        "The __init__ method initializes the MultiHeadAttention module.\n",
        "It takes two parameters: num_heads and head_size.\n",
        "It creates a list of num_heads instances of the Head class using nn.ModuleList. Each head has its own set of learnable parameters for the self-attention mechanism.\n",
        "The proj linear layer is used to linearly combine the outputs of the individual heads. The output dimension is n_embd to maintain consistency with the overall model architecture.\n",
        "Dropout is applied to the aggregated output for regularization.\n",
        "Forward Pass:\n",
        "\n",
        "The forward method takes an input tensor x and applies each head's self-attention mechanism independently.\n",
        "The outputs of the individual heads are concatenated along the last dimension (dim=-1).\n",
        "The concatenated output is linearly transformed using the proj linear layer.\n",
        "Dropout is applied to the aggregated output for regularization.\n",
        "The final output represents the result of applying multiple heads of self-attention to the input."
      ],
      "metadata": {
        "id": "XwRkOhG0dldZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "MlQ5-mS0B0lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') #calculates and prints the total number of parameters in the GPT language model (m)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGJLQwLxCTf1",
        "outputId": "333fa353-7d81-432a-e242-d088c170ad0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)#AdamW is a variant of the Adam optimizer that includes weight decay, which is a form\n",
        "# of regularization. The \"W\" in AdamW stands for \"Weight Decay.\""
      ],
      "metadata": {
        "id": "teJH64XoCWZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHA9DCczCfkk",
        "outputId": "82984892-88ed-4b22-a63f-d74d1b86ca3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2221, val loss 4.2306\n",
            "step 500: train loss 1.7600, val loss 1.9146\n",
            "step 1000: train loss 1.3903, val loss 1.5987\n",
            "step 1500: train loss 1.2644, val loss 1.5271\n",
            "step 2000: train loss 1.1835, val loss 1.4978\n",
            "step 2500: train loss 1.1233, val loss 1.4910\n",
            "step 3000: train loss 1.0718, val loss 1.4804\n",
            "step 3500: train loss 1.0179, val loss 1.5127\n",
            "step 4000: train loss 0.9604, val loss 1.5102\n",
            "step 4500: train loss 0.9125, val loss 1.5351\n",
            "step 4999: train loss 0.8589, val loss 1.5565\n",
            "\n",
            "But with prison, I will steal for the fimker.\n",
            "\n",
            "KING HENRY VI:\n",
            "To prevent it, as I love this country's cause.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "I thank bhop my follow. Walk ye were so?\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "My lord, I hearison! Who may love me accurse\n",
            "Some chold or flights then men shows to great the cur\n",
            "Ye cause who fled the trick that did princely action?\n",
            "Take my captiving sound, althoughts thy crown.\n",
            "\n",
            "RICHMOND NE:\n",
            "God neit will he not make it wise this!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Worthy Prince forth from Lord Claudio!\n",
            "\n",
            "Lo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop iterates max_iters times, where max_iters is the maximum number of training iterations specified in the hyperparameters.\n",
        "Every eval_interval iterations or at the last iteration (max_iters - 1), the model's performance is evaluated on both the training and validation sets.\n",
        "The estimate_loss function calculates the average loss over several evaluation iterations and returns a dictionary containing the training and validation losses.\n",
        "The losses are then printed to monitor the training progress.\n",
        "The model processes the input (xb) and target (yb) to compute the logits and the corresponding loss.\n",
        "The optimizer's gradients are zeroed with optimizer.zero_grad(set_to_none=True) to avoid accumulating gradients from previous iterations.\n",
        "Backpropagation is performed using loss.backward() to compute gradients.\n",
        "The optimizer then takes a step in the parameter space to minimize the loss using optimizer.step()After training, the model is used for text generation.\n",
        "A context tensor is initialized, and the generate method is called to generate text with a maximum of 500 new tokens.\n",
        "The generated sequence is then printed.\n",
        "This block essentially represents the core of the training process, including loss evaluation, parameter updates, and optional text generation. It's crucial for monitoring and improving the model during training."
      ],
      "metadata": {
        "id": "K_5-tNSJgNq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How Does Dropout Work?\n",
        "\n",
        "During training, at each update of the model's parameters, dropout randomly \"drops out\" (i.e., sets to zero) a subset of the neurons in the network.\n",
        "This means that, for a short period, certain neurons do not contribute to the forward pass or backward pass of a specific training example.\n",
        "The random dropout of neurons introduces a form of noise and prevents the network from becoming overly reliant on specific neurons or features. It encourages the network to learn more robust and generalizable representations.\n",
        "\n",
        "2-\n",
        "The FeedFoward class represents a feedforward neural network layer used in the Transformer model. Let's break down its structure and purpose:\n",
        "\n",
        "1. Initialization:\n",
        "\n",
        "The __init__ method is the constructor that defines the architecture of the feedforward layer.\n",
        "It takes n_embd as a parameter, which represents the embedding dimension, a key parameter in the Transformer model.\n",
        "2. Network Architecture:\n",
        "\n",
        "The feedforward layer consists of a simple neural network, defined using nn.Sequential.\n",
        "It starts with a linear (fully connected) layer that maps the input of dimension n_embd to an intermediate dimension of 4 * n_embd.\n",
        "This is followed by a Rectified Linear Unit (ReLU) activation function, introducing non-linearity to the network.\n",
        "The output of the ReLU activation is then passed through another linear layer that maps from the intermediate dimension back to the original embedding dimension, n_embd.\n",
        "Finally, a dropout layer is applied. Dropout is a regularization technique that randomly sets a fraction of input units to zero during training to prevent overfitting.\n",
        "3. Forward Pass:\n",
        "\n",
        "The forward method defines how the input x is processed through the feedforward layer during the forward pass.\n",
        "It simply passes the input x through the defined network (self.net), applying the linear transformations and non-linearities in sequence.\n",
        "4. Purpose:\n",
        "\n",
        "The purpose of this feedforward layer is to introduce non-linearity and learn complex patterns from the input embeddings.\n",
        "The intermediate dimension of 4 * n_embd allows the network to capture more intricate relationships in the data.\n",
        "Dropout is applied to regularize the network, preventing overfitting and improving generalization to unseen data.\n",
        "5. Analogy: Math Problem Solving\n",
        "\n",
        "Think of this like solving a math problem: you start with a simple equation, apply a non-linear transformation (like squaring), then simplify it further. Dropout is like occasionally skipping a step or introducing variability in your problem-solving approach to improve your understanding.\n",
        "In the context of the Transformer model, this feedforward layer is a crucial component within each transformer block, contributing to the model's ability to capture and learn complex patterns in sequential data.\n",
        "\n",
        "3-The Block class represents a single Transformer block, which is a fundamental building block of the Transformer model. Let's break down its structure and purpose:\n",
        "\n",
        "1. Initialization:\n",
        "\n",
        "The __init__ method is the constructor that defines the components of the Transformer block.\n",
        "It takes n_embd as the embedding dimension and n_head as the number of attention heads.\n",
        "head_size is calculated as n_embd // n_head, representing the size of each attention head.\n",
        "2. Components of the Transformer Block:\n",
        "\n",
        "Self-Attention (sa): This is an instance of the MultiHeadAttention class, which is responsible for capturing relationships between different positions of the input sequence.\n",
        "Feedforward (ffwd): This is an instance of the FeedForward class, a simple neural network layer that introduces non-linearity and captures complex patterns in the data.\n",
        "Layer Normalization (ln1, ln2): Two instances of layer normalization (nn.LayerNorm) are applied before and after the self-attention and feedforward components, respectively. Layer normalization helps stabilize the training process.\n",
        "3. Forward Pass:\n",
        "\n",
        "The forward method defines the forward pass of the Transformer block.\n",
        "It first applies self-attention (self.sa) to the input x after passing it through layer normalization (self.ln1(x)).\n",
        "The output is then added to the original input (x), creating a residual connection.\n",
        "After another layer normalization (self.ln2(x)), the result is passed through the feedforward component (self.ffwd).\n",
        "Again, the output is added to the previous result, creating another residual connection.\n",
        "The final output of the block is returned.\n",
        "4. Purpose:\n",
        "\n",
        "The Transformer block enables the model to capture hierarchical and long-range dependencies in the input sequence.\n",
        "Self-attention allows the model to focus on different parts of the sequence, and the feedforward component captures complex patterns.\n",
        "Layer normalization and residual connections contribute to stable and efficient training.\n",
        "5. Analogy: Team Collaboration\n",
        "\n",
        "Think of this like a team collaboration: self-attention is like team members communicating and sharing information, feedforward is like individual team members working on specific tasks, and layer normalization and residuals are like team members maintaining a consistent and stable workflow.\n",
        "In the context of the overall Transformer model, stacking multiple such blocks allows the model to learn and represent intricate patterns in sequential data, making it effective for various natural language processing tasks."
      ],
      "metadata": {
        "id": "M_CSAmOFeDXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPTLanguageModel class defines the architecture of the GPT (Generative Pre-trained Transformer) language model. Let's go through its components and their functionalities:\n",
        "\n",
        "1. Token and Position Embeddings:\n",
        "\n",
        "self.token_embedding_table: Embedding layer for token embeddings. Each token in the vocabulary is represented as an embedding vector.\n",
        "self.position_embedding_table: Embedding layer for position embeddings. It assigns each position in the input sequence a unique embedding vector. This helps the model understand the sequential order of tokens.\n",
        "2. Transformer Blocks:\n",
        "\n",
        "self.blocks: A stack of Transformer blocks. The number of blocks is determined by the n_layer hyperparameter. Each block consists of self-attention, feedforward networks, and layer normalization.\n",
        "3. Layer Normalization and Final Layer:\n",
        "\n",
        "self.ln_f: Layer normalization applied to the final output of the Transformer blocks. It helps stabilize and normalize the outputs.\n",
        "self.lm_head: Linear layer that produces logits for the next token based on the final output of the model.\n",
        "4. Initialization:\n",
        "\n",
        "The model uses the nn.Module.apply method to initialize the weights of linear and embedding layers. The initialization is performed using a normal distribution with a mean of 0 and a standard deviation of 0.02. This is a common practice for better convergence during training.\n",
        "5. Initialization Method (_init_weights):\n",
        "\n",
        "The _init_weights method is used to initialize the weights of linear and embedding layers.\n",
        "For linear layers (nn.Linear), weights are initialized from a normal distribution, and biases are set to zero.\n",
        "For embedding layers (nn.Embedding), weights are also initialized from a normal distribution.\n",
        "6. Purpose:\n",
        "\n",
        "The GPT language model processes input sequences by embedding tokens and positions, passing them through multiple Transformer blocks, and generating logits for the next token.\n",
        "The model is trained to minimize the cross-entropy loss between predicted logits and actual tokens in the training data.\n",
        "7. Model Initialization:\n",
        "\n",
        "An instance of this model is created using model = GPTLanguageModel() and moved to the specified device (CPU or GPU).\n",
        "The total number of parameters in the model is printed to give an idea of the model size.\n",
        "This architecture is fundamental to the success of GPT models, enabling them to capture complex patterns and dependencies in sequential data.The forward method of the GPTLanguageModel class defines how input sequences are processed during both training and inference. Additionally, the generate method is designed for generating new sequences. Let's break down these methods:\n",
        "\n",
        "1. Forward Method (forward):\n",
        "\n",
        "Input:\n",
        "idx: Tensor of shape (B, T), representing a batch of input sequences where each element is an index corresponding to a token in the vocabulary.\n",
        "targets: Optional tensor of shape (B, T), representing the target tokens for training.\n",
        "Processing Steps:\n",
        "Token and Position Embedding: Embeds tokens and adds positional embeddings.\n",
        "Transformer Blocks: Passes the embedded sequences through the stack of Transformer blocks (self.blocks).\n",
        "Layer Normalization: Applies layer normalization (self.ln_f) to the output of Transformer blocks.\n",
        "Logits Generation: Produces logits for the next token using a linear layer (self.lm_head).\n",
        "Loss Computation (if targets provided): Computes the cross-entropy loss between predicted logits and target tokens.\n",
        "2. Generation Method (generate):\n",
        "\n",
        "Input:\n",
        "idx: Tensor of shape (B, T), representing a batch of input sequences for generating new tokens.\n",
        "max_new_tokens: Maximum number of new tokens to generate.\n",
        "Processing Steps:\n",
        "Crop Sequence: Keeps only the last block_size tokens in the input sequence.\n",
        "Get Predictions: Generates logits for the next token by calling the forward method.\n",
        "Focus on Last Time Step: Extracts logits corresponding to the last time step.\n",
        "Softmax: Applies softmax to obtain a probability distribution over the vocabulary.\n",
        "Sampling: Uses multinomial sampling to select the next token based on the probability distribution.\n",
        "Update Sequence: Appends the sampled token to the running sequence.\n",
        "Repeat: Repeats the process for the specified number of max_new_tokens.\n",
        "These methods demonstrate how the GPT language model can be used for both training (with targets provided) and generation (without targets). During generation, the model autoregressively predicts the next token and updates the input sequence for subsequent predictions. This mechanism allows the model to generate coherent sequences of text."
      ],
      "metadata": {
        "id": "AnpUbxzRe-7m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCGJwDTSeFhT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}